\begin{abstract}{english}
	Graph neural networks (GNNs) present a framework for representation learning on graphs that has been dominant for the past several years. The main strength of GNNs lies in the fact that they can simultaneously learn both from node-related attributes as well as relations between nodes, represented by edges. In tasks leading to large graphs, a GNN often requires significant computational resources to achieve its superior performance. In order to reduce this computational cost, methods allowing for a flexible balance between complexity and performance could be useful. In this work, we propose a simple, scalable, task-aware graph pre-processing procedure that allows us to obtain a reduced graph in such a manner that the GNN achieves a predefined desired performance level on the downstream task in question. In addition, the proposed pre-processing allows for fitting the reduced graph and GNN into given memory/computational resources.

	The pre-processing procedure is built on the elementary operation of graph edge contraction. By contracting the edges of the graph one-by-one, a sequence of graphs is obtained, starting with the original one and ending with a graph with no edges and one node per each connected component of the original graph. For each graph in this sequence, a tuple (performance, complexity) can be obtained, where in our work, performance is measured as the accuracy of a classifier on the given downstream task and complexity is measured as the number of nodes in the particular graph. Using these values, the sequence of graphs generated by the pre-processing procedure traces a path in the performance-complexity space. The aim of our work is to study the properties of such a path, with a particular interest in finding a point with the best performance for a given complexity budget, or, conversely, finding the point of lowest complexity for a given required minimal performance.

	The edge contraction procedure is driven by an ordering of the edges of the original graph, which in turn defines the aforementioned sequence of graphs. In our work, we define this ordering by measuring the similarity of the predictive posterior distribution of labels of the nodes incident on the edge in question. The choice of a similarity measure is explored experimentally, together with several ways of computing the predictive posterior distribution based on a simplified model specific to the given task.

	Additionally, when contracting an edge, a feature aggregation strategy must be defined, as well as a label aggregation strategy. A simple weighted average was used, where the weights are given for the feature aggregation strategy by the number of nodes from the original graph that are represented by a given node. For the label aggregation strategy, similarly, a weighted average of the label distributions of both nodes was used, with the weights representing the number of training nodes represented by a given node. Moreover, when an intermediary graph is to be used for predictions on the original graph, a label refinement strategy is needed in the cases where multiple nodes of the original graph are represented by one node in the intermediary graph. For this strategy, a simple copying of labels was used. This choice of label refinement, however, defines an upper bound on the performance that can be obtained on any given graph in the sequence.

	The proposed preprocessing is evaluated and compared with several reference scenarios on conventional GNN benchmark datasets. The performance of the algorithm is compared to the theoretical upper bound defined by the label refinement strategy and the impact of the edge ordering procedure on the performance-complexity characteristics of the algorithm is studied. The main result of this work is that the proposed pre-processing allows for a significant reduction in the number of nodes of a given graph (in some cases, up to 50\%) without a major impact on the performance.
\end{abstract}

\begin{keywords}{english}
	Graph neural network, Complexity reduction, Hierarchical clustering, Big data
\end{keywords}

\begin{abstract}{czech}
	Grafové neuronové sítě (GNN) představují v posledních letech dominantní nástroj pro reprezentační učení na grafech. Hlavním přínosem GNN je fakt, že se dokáží učit zároveň z příznaků vrcholů a jejich vzájemných vztahů, reprezentovaných hranami. Při řešení úloh vedoucích na velké grafy potřebují GNN často velké množství výpočetních zdrojů aby dosáhly svého vysokého výkonu. Za účelem snížení těchto vysokých výpočetních nároků mohou být užitečné metody dovolující flexibilní kompromis mezi kvalitou předpovědi a výpočetní složitostí. V této práci navrhujeme jednoduchý, škálovatelný, na úloze závisející algoritmus pro předzpracování grafů tak, aby výsledný graf byl zjednodušený takovým způsobem, aby neuronová síť dosáhla požadovaného výkonu na předdefinované cílové úloze. Navrhované předzpracování navíc umožňuje přizpůsobení redukovaného grafu a modelu dostupným výpočetním zdrojům a paměti.

	Algoritmus pro předzpracování je postaven na základní operaci kontrakce hrany grafu. Při kontrakci hran jedné po druhé dostáváme posloupnost grafů, počínaje originálním a konče grafem bez hran, kde každá komponenta původního grafu je kontrahována do jednoho vrcholu. Pro každý graf v této posloupnosti lze určit dvojici (výkonnost, složitost). V tomto díle je výkonnost měřena jako přesnost klasifikátoru na dané, předem specifikované úloze a složitost je měřena počtem vrcholů zjednodušeného grafu. Pomocí těchto hodnot lze posloupnost grafů generovanou algoritmem vykreslit jako cestu v prostoru výkonnost-složitost. Cílem této práce je studium vlastností takovéto cesty s mimořádnou pozorností na hledání bodu s nejlepším výkonem pro dané výpočetní možnosti nebo naopak bodu s nejmenší výpočetní složitostí při dosažení daného minimálního výkonu.

	Algoritmus zjednodušování grafu je postaven na seřazení hran původního grafu, které skrze jejich kontrakci vyústí ve výše zmíněnou posloupnost grafů. V této práci definujeme takové řazení hran pomocí podobnosti prediktivního posteriorního pravděpodobnostního rozdělení tříd vrcholů přiléhajících na danou hranu. Volba podobnostní míry je zkoumána experimentálně, stejně tak jako několik způsobů výpočtu prediktivního posteriorního pravděpodobnostního rozdělení pomocí jednoduchých modelů specifických pro danou úlohu.

	Při kontrakci hran musí navíc být specifikovány strategie pro agregaci příznaků a tříd vrcholů grafu. V této práci bylo použit vážený průměr, kde pro agregaci příznaků byly váhy určeny jako počet vrcholů původního grafu, které jsou reprezentovány daným vrcholem. Pro agregaci tříd byl obdobně použit vážený průměr pravděpodobnostních rozdělení tříd obou vrcholů, kde váhy byly určeny jako počet vrcholů z trénovací sady, které jsou reprezentovány daným vrcholem. V případě, kdy je graf z posloupnosti použit pro klasifikaci na původním grafu je navíc zapotřebí definovat strategii pro zjemňování tříd v situaci, kde jeden vrchol daného grafu odpovídá několika vrcholům původního grafu, potenciálně s různými třídami. Jako tato strategie bylo použité jednoduché kopírování tříd. Tato volba zjemňování tříd definuje horní mez výkonu, který může být dosažený pro daný graf z posloupnosti.

	Navrhované předzpracování grafu je vyhodnoceno a porovnáno s několika referenčními scénáři na datasetech bězně využívaných pro vyhodnocování grafových neuronových sítí. Výkon algoritmu je srovnán s teoretickou horní mezí určenou strategií zjemňování tříd a je studován vliv řazení hran grafu na charakteristiku výkonu a složitosti. Hlavním výsledkem této práce je, že navrhované předzpracování grafu umožňuje výrazné snížení složitosti (v některých případech až o 50\% vrcholů) bez významnějšího dopadu na výkon.
\end{abstract}

\begin{keywords}{czech}
	Grafová neuronová síť, Redukce složitosti, Hierarchické shlukování, Big data
\end{keywords}
