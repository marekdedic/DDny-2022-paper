\begin{abstract}{english}
	Graph neural networks (GNNs) present a framework for representation learning on graphs that has been dominant for the past several years. The main strength of GNNs lies in the fact that they can simultaneously learn both from node-related attributes as well as relations between nodes, represented by edges. In tasks leading to large graphs, a GNN often requires significant computational resources to achieve its superior performance. In order to reduce this computational cost, methods allowing for a flexible balance between complexity and performance could be useful. In this work, we propose a simple, scalable, task-aware graph pre-processing procedure that allows us to obtain a reduced graph in such a manner that the GNN achieves a predefined desired performance level on the downstream task in question. In addition, the proposed pre-processing allows for fitting the reduced graph and GNN into given memory/computational resources.

	The pre-processing procedure is built on the elementary operation of graph edge contraction. By contracting the edges of the graph one-by-one, a sequence of graphs is obtained, starting with the original one and ending with a graph with no edges and one node per each connected component of the original graph. For each graph in this sequence, a tuple (performance, complexity) can be obtained, where in our work, performance is measured as the accuracy of a classifier on the given downstream task and complexity is measured as the number of nodes in the particular graph. Using these values, the sequence of graphs generated by the pre-processing procedure traces a path in the performance-complexity space. The aim of our work is to study the properties of such a path, with a particular interest in finding a point with the best performance for a given complexity budget, or, conversely, finding the point of lowest complexity for a given required minimal performance.

	The edge contraction procedure is driven by an ordering of the edges of the original graph, which in turn defines the aforementioned sequence of graphs. In our work, we define this ordering by measuring the similarity of the predictive posterior distribution of labels of the nodes incident on the edge in question. The choice of a similarity measure is explored experimentally, together with several ways of computing the predictive posterior distribution based on a simplified model specific to the given task.

	Additionally, when contracting an edge, a feature aggregation strategy must be defined, as well as a label aggregation strategy. A simple weighted average was used, where the weights are given for the feature aggregation strategy by the number of nodes from the original graph that are represented by a given node. For the label aggregation strategy, similarly, a weighted average of the label distributions of both nodes was used, with the weights representing the number of training nodes represented by a given node. Moreover, when an intermediary graph is to be used for predictions on the original graph, a label refinement strategy is needed in the cases where multiple nodes of the original graph are represented by one node in the intermediary graph. For this strategy, a simple copying of labels was used. This choice of label refinement, however, defines an upper bound on the performance that can be obtained on any given graph in the sequence.

	The proposed preprocessing is evaluated and compared with several reference scenarios on conventional GNN benchmark datasets. The performance of the algorithm is compared to the theoretical upper bound defined by the label refinement strategy and the impact of the edge ordering procedure on the performance-complexity characteristics of the algorithm is studied. The main result of this work is that the proposed pre-processing allows for a significant reduction in the number of nodes of a given graph (in some cases, up to 50\%) without a major impact on the performance.
\end{abstract}

\begin{keywords}{english}
	Graph neural network, Complexity reduction, Hierarchical clustering, Big data
\end{keywords}

\begin{abstract}{czech}
	Toto je ukázkový příspěvek vytvořený jako součást pokynů pro doktorandy pro sazbu příspěvků do sborníku workshopu Doktorandské dny.
\end{abstract}

\begin{keywords}{czech}
	Grafová neuronová síť, Redukce složitosti, Hierarchické shlukování, Bid data
\end{keywords}
