\begin{abstract}{english}
	Graph neural networks (GNN) present a dominant framework for representation learning on graphs for the past several years. The main strength of GNNs lies in the fact that they can simultaneously learn from both node related attributes and relations between nodes, represented by edges. In tasks leading to large graphs, GNN often requires significant computational resources to achieve its superior performance. In order to reduce the computational cost, methods allowing for a flexible balance between complexity and performance could be useful. In this work, we propose a simple scalable task-aware graph preprocessing procedure allowing us to obtain a reduced graph such as GNN achieves a given desired performance on the downstream task. In addition, the proposed preprocessing allows for fitting the reduced graph and GNN into a given memory/computational resources. The proposed preprocessing is evaluated and compared with several reference scenarios on conventional GNN benchmark datasets.
\end{abstract}

\begin{keywords}{english}
	first, second, third
\end{keywords}

\begin{abstract}{czech}
	Toto je ukázkový příspěvek vytvořený jako součást pokynů pro doktorandy pro sazbu příspěvků do sborníku workshopu Doktorandské dny.
\end{abstract}

\begin{keywords}{czech}
	první, druhé a třetí
\end{keywords}
