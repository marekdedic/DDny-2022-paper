\begin{abstract}{english}
	Graph neural networks (GNNs) present a framework for representation learning on graphs that has been dominant for the past several years. The main strength of GNNs lies in the fact that they can simultaneously learn both from node-related attributes as well as relations between nodes, represented by edges. In tasks leading to large graphs, a GNN often requires significant computational resources to achieve its superior performance. In order to reduce this computational cost, methods allowing for a flexible balance between complexity and performance could be useful. In this work, we propose a simple, scalable, task-aware graph pre-processing procedure that allows us to obtain a reduced graph in such a manner that the GNN achieves a predefined desired performance level on the downstream task in question. In addition, the proposed pre-processing allows for fitting the reduced graph and GNN into given memory/computational resources. The proposed preprocessing is evaluated and compared with several reference scenarios on conventional GNN benchmark datasets.
\end{abstract}

\begin{keywords}{english}
	first, second, third
\end{keywords}

\begin{abstract}{czech}
	Toto je ukázkový příspěvek vytvořený jako součást pokynů pro doktorandy pro sazbu příspěvků do sborníku workshopu Doktorandské dny.
\end{abstract}

\begin{keywords}{czech}
	první, druhé a třetí
\end{keywords}
